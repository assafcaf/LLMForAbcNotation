{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-03T14:38:32.096430200Z",
     "start_time": "2023-08-03T14:38:31.506157100Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer, SentencePieceBPETokenizer, BertWordPieceTokenizer\n",
    "from transformers import GPT2Tokenizer, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\studies\\\\datasets\\\\abc_dataset\\\\test.txt', 'C:\\\\studies\\\\datasets\\\\abc_dataset\\\\test_yandex.txt', 'C:\\\\studies\\\\datasets\\\\abc_dataset\\\\train.txt', 'C:\\\\studies\\\\datasets\\\\abc_dataset\\\\train_yandex.txt']\n"
     ]
    }
   ],
   "source": [
    "data_dir = r\"C:\\studies\\datasets\\abc_dataset\"\n",
    "paths = [str(x) for x in Path(data_dir).glob('*.txt')]\n",
    "print(paths)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T14:43:16.532893700Z",
     "start_time": "2023-08-03T14:43:16.521577500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "tokenizer_name = 'BertWordPieceTokenizer'\n",
    "tokenizer = eval(f\"{tokenizer_name}()\")\n",
    "tokenizer.train(files=paths, vocab_size=5000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\"\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T14:45:38.963539200Z",
     "start_time": "2023-08-03T14:43:17.587778Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "['tokenizers\\\\ByteLevelBPETokenizer\\\\vocab.json',\n 'tokenizers\\\\ByteLevelBPETokenizer\\\\merges.txt']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pth = os.path.join('tokenizers', tokenizer_name)\n",
    "os.makedirs(pth, exist_ok=True)\n",
    "tokenizer.save_model(pth)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T14:46:11.906716700Z",
     "start_time": "2023-08-03T14:46:11.884258400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151 250\n",
      "Encoding(num_tokens=151, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "['S', ':', '2', 'Ġ', '<', 'nl', '>', 'B', ':', '9', 'Ġ', '<', 'nl', '>', 'E', ':', '5', 'Ġ', '<', 'nl', '>', 'B', ':', '9', 'Ġ', '<', 'nl', '><', 'sep', '>', 'X', ':', '1', 'Ġ', '<', 'nl', '>', 'L', ':', '1', '/', '8', 'Ġ', '<', 'nl', '>', 'Q', ':', '1', '/', '8', '=', '200', 'Ġ', '<', 'nl', '>', 'M', ':', '2', '/', '2', 'Ġ', '<', 'nl', '>', 'K', ':', 'D', '<', 'sep', '>\"', 'D', '\"', 'ĠF', '3', 'ĠG', 'ĠFEDE', 'Ġ|', 'ĠFAAB', 'ĠAFED', 'Ġ|', 'Ġd', '2', 'Ġ(', '3', 'efg', '\"', 'G', '\"', 'Ġfdec', 'Ġ|\"', 'D', '\"', 'ĠdBAF', '\"', 'A', '\"', 'ĠBE', 'ĠE', '2', 'Ġ|\"', 'D', '\"', 'ĠF', '3', 'ĠG', 'ĠFEDE', 'Ġ|', 'ĠFAAB', 'ĠAFED', 'Ġ|', 'Ġ', 'Ġd', '2', 'Ġ(', '3', 'efg', '\"', 'G', '\"', 'Ġfdec', 'Ġ|', '1', '\"', 'A', '\"', 'ĠdBAG', '\"', 'D', '\"', 'ĠFD', 'ĠD', '2', 'Ġ:|', '2', '\"', 'A', '\"', 'ĠdBAG', '\"', 'D', '\"', 'ĠFDFA', 'Ġ|:\"', 'D', '\"', 'Ġd', '2', 'ĠAG', 'Ċ']\n",
      "[55, 30, 22, 225, 32, 274, 34, 38, 30, 29, 225, 32, 274, 34, 41, 30, 25, 225, 32, 274, 34, 38, 30, 29, 225, 32, 274, 354, 296, 34, 60, 30, 21, 225, 32, 274, 34, 48, 30, 21, 19, 28, 225, 32, 274, 34, 53, 30, 21, 19, 28, 33, 1401, 225, 32, 274, 34, 49, 30, 22, 19, 22, 225, 32, 274, 34, 47, 30, 40, 32, 296, 836, 40, 6, 312, 23, 290, 2415, 269, 2304, 1782, 269, 284, 22, 310, 23, 703, 6, 43, 6, 1008, 319, 40, 6, 942, 6, 37, 6, 824, 317, 22, 319, 40, 6, 312, 23, 290, 2415, 269, 2304, 1782, 269, 225, 284, 22, 310, 23, 703, 6, 43, 6, 1008, 269, 21, 6, 37, 6, 1031, 6, 40, 6, 570, 322, 22, 353, 22, 6, 37, 6, 1031, 6, 40, 6, 1509, 829, 40, 6, 284, 22, 512, 203]\n",
      "S:2 <nl>B:9 <nl>E:5 <nl>B:9 <nl><sep>X:1 <nl>L:1/8 <nl>Q:1/8=200 <nl>M:2/2 <nl>K:D<sep>\"D\" F3 G FEDE | FAAB AFED | d2 (3efg\"G\" fdec |\"D\" dBAF\"A\" BE E2 |\"D\" F3 G FEDE | FAAB AFED |  d2 (3efg\"G\" fdec |1\"A\" dBAG\"D\" FD D2 :|2\"A\" dBAG\"D\" FDFA |:\"D\" d2 AG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "key = \"\"\"S:2 <nl>B:9 <nl>E:5 <nl>B:9 <nl><sep>X:1 <nl>L:1/8 <nl>Q:1/8=200 <nl>M:2/2 <nl>K:D<sep>\"D\" F3 G FEDE | FAAB AFED | d2 (3efg\"G\" fdec |\"D\" dBAF\"A\" BE E2 |\"D\" F3 G FEDE | FAAB AFED |  d2 (3efg\"G\" fdec |1\"A\" dBAG\"D\" FD D2 :|2\"A\" dBAG\"D\" FDFA |:\"D\" d2 AG\n",
    "\"\"\"\n",
    "outputs = tokenizer.encode(key)\n",
    "print(len(outputs), len(key))\n",
    "print(outputs)\n",
    "print(outputs.tokens)\n",
    "print(outputs.ids)\n",
    "print(tokenizer.decode(outputs.ids))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T14:45:58.996417500Z",
     "start_time": "2023-08-03T14:45:58.948361Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5001\n"
     ]
    }
   ],
   "source": [
    "gpt2_tokenizer = DistilBertTokenizer.from_pretrained(pth, bos_token=\"<s>\", eos_token=\"</s>\", pad_token=\"<pad>\", mask_token=\"<mask>\")\n",
    "# outputs = gpt2_tokenizer.encode(key)\n",
    "# print(outputs)\n",
    "# print(tokenizer.decode(outputs))\n",
    "print(len(gpt2_tokenizer))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T11:59:02.898462100Z",
     "start_time": "2023-08-03T11:59:02.887827500Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
